# å¤§è¯­è¨€æ¨¡å‹æ¨¡å—
import json
from vlm import *

try:
    from letta import create_client, LLMConfig, EmbeddingConfig
    from letta.schemas.memory import ChatMemory
except:
    pass

lmstudio_history, ollama_history, rwkv_history, custom_history, spark_history, glm_history, lyww_history, ds_history, qwen_history, internlm_history = [], [], [], [], [], [], [], [], [], []
sf_url = "https://api.siliconflow.cn/v1"


def chat_preprocess(msg):  # é¢„å¤„ç†
    try:
        content = "å›¾åƒè¯†åˆ«å·²å…³é—­"
        if (
                "å±å¹•" in msg or "ç”»é¢" in msg or "å›¾ç‰‡" in msg or "çœ‹åˆ°" in msg or "çœ‹è§" in msg or "ç…§ç‰‡" in msg or "æ‘„åƒå¤´" in msg) and img_menu.get() != "å…³é—­å›¾åƒè¯†åˆ«":
            if "å±å¹•" in msg or "ç”»é¢" in msg or "å›¾ç‰‡" in msg:
                if img_menu.get() == "GLM-4V-Flash":
                    content = glm_4v_screen(msg)
                elif img_menu.get() == "æœ¬åœ°Ollama VLM":
                    content = ollama_vlm_screen(msg)
                elif img_menu.get() == "æœ¬åœ°QwenVLæ•´åˆåŒ…":
                    content = qwen_vlm_screen(msg)
                elif img_menu.get() == "æœ¬åœ°GLM-Væ•´åˆåŒ…":
                    content = glm_v_screen(msg)
                elif img_menu.get() == "æœ¬åœ°Janusæ•´åˆåŒ…":
                    content = janus_screen(msg)
                elif img_menu.get() == "è‡ªå®šä¹‰API-VLM":
                    content = custom_vlm_screen(msg)
                notice(f"{mate_name}æ•è·äº†å±å¹•ï¼Œè°ƒç”¨[ç”µè„‘å±å¹•è¯†åˆ«]")
            elif "çœ‹åˆ°" in msg or "çœ‹è§" in msg or "ç…§ç‰‡" in msg or "æ‘„åƒå¤´" in msg:
                if img_menu.get() == "GLM-4V-Flash":
                    content = glm_4v_cam(msg)
                elif img_menu.get() == "æœ¬åœ°Ollama VLM":
                    content = ollama_vlm_cam(msg)
                elif img_menu.get() == "æœ¬åœ°QwenVLæ•´åˆåŒ…":
                    content = qwen_vlm_cam(msg)
                elif img_menu.get() == "æœ¬åœ°GLM-Væ•´åˆåŒ…":
                    content = glm_v_cam(msg)
                elif img_menu.get() == "æœ¬åœ°Janusæ•´åˆåŒ…":
                    content = janus_cam(msg)
                elif img_menu.get() == "è‡ªå®šä¹‰API-VLM":
                    content = custom_vlm_cam(msg)
                notice(f"{mate_name}æ‹äº†ç…§ç‰‡ï¼Œè°ƒç”¨[æ‘„åƒå¤´è¯†åˆ«]")
        else:
            content = chat_llm(prompt, msg)
            if think_filter_switch == "å¼€å¯":
                content = content.split("</think>")[-1].strip()
            notice(f"æ”¶åˆ°{mate_name}å›å¤")
        return content
    except Exception as e:
        notice(f"å›¾åƒè¯†åˆ«å¼•æ“é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")
        return "å›¾åƒè¯†åˆ«å¼•æ“é…ç½®é”™è¯¯"


def chat_llm(tishici, msg):  # å¤§è¯­è¨€æ¨¡å‹èŠå¤©
    try:
        if llm_menu.get() == "è®¯é£æ˜Ÿç«Lite":
            spark_client = OpenAI(base_url="https://spark-api-open.xf-yun.com/v1", api_key=spark_key)
            spark_history.append({"role": "user", "content": f"{tishici}ã€‚æˆ‘çš„é—®é¢˜æ˜¯ï¼š{msg}"})
            messages = []
            messages.extend(spark_history)
            completion = spark_client.chat.completions.create(model="general", messages=messages)
            spark_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "GLM-4-Flash":
            glm_client = OpenAI(base_url=glm_url, api_key=glm_key)
            glm_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(glm_history)
            completion = glm_client.chat.completions.create(model="glm-4-flash", messages=messages)
            glm_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "DeepSeek-R1-7B":
            ds_client = OpenAI(base_url=sf_url, api_key=sf_key)
            ds_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(ds_history)
            completion = ds_client.chat.completions.create(model="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                                                           messages=messages)
            ds_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "æ€ç»´é“¾Marco-o1":
            marco_client = OpenAI(base_url=sf_url, api_key=sf_key)
            messages = [{"role": "system",
                         "content": f"{tishici}ã€‚å½“ä½ å›ç­”é—®é¢˜æ—¶ï¼Œä½ çš„æ€è€ƒåº”è¯¥åœ¨<think>å†…å®Œæˆï¼Œ<answer>å†…è¾“å‡ºä½ çš„ç»“æœã€‚<think>åº”è¯¥å°½å¯èƒ½æ˜¯ä¸­æ–‡"},
                        {"role": "user", "content": msg}]
            completion = marco_client.chat.completions.create(model="AIDC-AI/Marco-o1", messages=messages)
            result = completion.choices[0].message.content
            result = result.replace("<answer>", "").replace("</answer>", "")
            return result
        elif llm_menu.get() == "é›¶ä¸€ä¸‡ç‰©1.5-9B":
            lyww_client = OpenAI(base_url=sf_url, api_key=sf_key)
            lyww_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(lyww_history)
            completion = lyww_client.chat.completions.create(model="01-ai/Yi-1.5-9B-Chat-16K",
                                                             messages=messages)
            lyww_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "é€šä¹‰åƒé—®2.5-7B":
            qwen_client = OpenAI(base_url=sf_url, api_key=sf_key)
            qwen_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(qwen_history)
            completion = qwen_client.chat.completions.create(model="Qwen/Qwen2.5-7B-Instruct",
                                                             messages=messages)
            qwen_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "InternLM2.5-7B":
            internlm_client = OpenAI(base_url=sf_url, api_key=sf_key)
            internlm_history.append({"role": "user", "content": msg})
            messages = [{"role": "system", "content": tishici}]
            messages.extend(internlm_history)
            completion = internlm_client.chat.completions.create(model="internlm/internlm2_5-7b-chat",
                                                                 messages=messages)
            internlm_history.append({"role": "assistant", "content": completion.choices[0].message.content})
            return completion.choices[0].message.content
        elif llm_menu.get() == "æœ¬åœ°Qwenæ•´åˆåŒ…":
            api = f"http://{local_server_ip}:8088/llm/?p={tishici}&q={msg}"
            try:
                res = rq.get(api).json()["answer"]
                return res
            except Exception as e:
                return f"æœ¬åœ°Qwenæ•´åˆåŒ…APIæœåŠ¡å™¨æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{str(e)[0:100]}"
        elif llm_menu.get() == "æœ¬åœ°LM Studio":
            try:
                lmstudio_client = OpenAI(base_url=f"http://{local_server_ip}:{lmstudio_port}/v1", api_key="lm-studio")
                lmstudio_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(lmstudio_history)
                completion = lmstudio_client.chat.completions.create(model="", messages=messages)
                lmstudio_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                return completion.choices[0].message.content
            except Exception as e:
                return f"æœ¬åœ°LM Studioè½¯ä»¶æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°Ollama":
            try:
                try:
                    rq.get(f'http://{local_server_ip}:11434')
                except:
                    Popen(f"ollama pull {ollama_model_name}", shell=False)
                ollama_client = Client(host=f'http://{local_server_ip}:11434')
                ollama_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(ollama_history)
                response = ollama_client.chat(model=ollama_model_name, messages=messages)
                ollama_history.append({"role": "assistant", "content": response['message']['content']})
                return response['message']['content']
            except Exception as e:
                return f"æœ¬åœ°OllamaæœåŠ¡æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°RWKVè¿è¡Œå™¨":
            try:
                rwkv_client = OpenAI(base_url=f"http://{local_server_ip}:8000/v1", api_key="rwkv")
                rwkv_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(rwkv_history)
                completion = rwkv_client.chat.completions.create(model="rwkv", messages=messages)
                rwkv_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                return completion.choices[0].message.content
            except Exception as e:
                return f"æœ¬åœ°RWKV Runnerè½¯ä»¶æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "æœ¬åœ°OpenVINO":
            api = f"http://{local_server_ip}:8087/openvino/?p={tishici}&q={msg}"
            try:
                res = rq.get(api).json()["answer"]
                return res
            except Exception as e:
                return f"æœ¬åœ°OpenVINOæ•´åˆåŒ…APIæœåŠ¡å™¨æœªå¼€å¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{str(e)[0:100]}"
        elif llm_menu.get() == "æœ¬åœ°DifyçŸ¥è¯†åº“":
            try:
                res = chat_dify(msg)
                return res
            except Exception as e:
                return f"æœ¬åœ°DifyçŸ¥è¯†åº“é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "AnythingLLM":
            try:
                res = chat_anything_llm(msg)
                return res
            except Exception as e:
                return f"æœ¬åœ°AnythingLLMçŸ¥è¯†åº“é…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
        elif llm_menu.get() == "Lettaé•¿æœŸè®°å¿†":
            res = chat_letta(msg)
            return res
        else:
            try:
                custom_client = OpenAI(base_url=custom_url, api_key=custom_key)
                custom_history.append({"role": "user", "content": msg})
                messages = [{"role": "system", "content": tishici}]
                messages.extend(custom_history)
                completion = custom_client.chat.completions.create(model=custom_model, messages=messages)
                custom_history.append({"role": "assistant", "content": completion.choices[0].message.content})
                return completion.choices[0].message.content
            except Exception as e:
                return f"è‡ªå®šä¹‰APIé…ç½®é”™è¯¯ï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"
    except Exception as e:
        notice(f"{llm_menu.get()}ä¸å¯ç”¨ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®æ­£ç¡®é…ç½®äº‘ç«¯AI Keyï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}")
        return f"{llm_menu.get()}ä¸å¯ç”¨ï¼Œè¯·å‰å¾€è½¯ä»¶è®¾ç½®æ­£ç¡®é…ç½®äº‘ç«¯AI Keyï¼Œé”™è¯¯è¯¦æƒ…ï¼š{e}"


def chat_dify(msg):  # DifyçŸ¥è¯†åº“
    headers = {"Authorization": f"Bearer {dify_key}", "Content-Type": "application/json"}
    data = {"query": msg, "inputs": {}, "response_mode": "blocking", "user": username, "conversation_id": None}
    res = rq.post(f"http://{dify_ip}/v1/chat-messages", headers=headers, data=json.dumps(data))
    res = res.json()['answer'].strip()
    return res


def chat_anything_llm(msg):  # AnythingLLMçŸ¥è¯†åº“
    url = f"http://{local_server_ip}:3001/api/v1/workspace/{anything_llm_ws}/chat"
    headers = {"Authorization": f"Bearer {anything_llm_key}", "Content-Type": "application/json"}
    data = {"message": msg}
    res = rq.post(url, json=data, headers=headers)
    return res.json().get("textResponse")


def chat_letta(msg):  # Lettaé•¿æœŸè®°å¿†
    answer = "Lettaé•¿æœŸè®°å¿†æœåŠ¡æ‹¥æŒ¤ï¼Œè¯·ä¸€æ®µæ—¶é—´åå†è¯•"
    try:
        client = create_client()
        with open('data/db/letta.db', 'r', encoding='utf-8') as f:
            agent_state_id = f.read()
        if len(agent_state_id) < 10:
            agent_state = client.create_agent(
                memory=ChatMemory(persona=prompt, human=f"Name: {username}"),
                llm_config=LLMConfig.default_config(model_name="letta"),
                embedding_config=EmbeddingConfig.default_config(model_name="text-embedding-ada-002"))
            with open('data/db/letta.db', 'w', encoding='utf-8') as f:
                f.write(agent_state.id)
            agent_state_id = agent_state.id
        response = client.send_message(
            agent_id=agent_state_id, role="user", message=f"[{current_time()}]{msg}")
        result = response.messages
        for message in result:
            if message.message_type == 'tool_call_message':
                function_arguments = message.tool_call.arguments
                if function_arguments:
                    arguments_dict = json.loads(function_arguments)
                    answer = arguments_dict.get("message")
                    break
    except:
        answer = "Lettaé•¿æœŸè®°å¿†å‡ºç°å…¼å®¹æ€§é—®é¢˜æš‚ä¸å¯ç”¨ï¼Œå¯æ›´æ¢å…¶ä»–å¯¹è¯æ¨¡å‹"
    return answer


def clear_chat():  # æ¸…é™¤å¯¹è¯è®°å½•
    global lmstudio_history, ollama_history, rwkv_history, custom_history, spark_history, glm_history, lyww_history, ds_history, qwen_history, internlm_history
    if messagebox.askokcancel(f"æ¸…é™¤{mate_name}çš„è®°å¿†å’ŒèŠå¤©è®°å½•",
                              f"æ‚¨ç¡®å®šè¦æ¸…é™¤{mate_name}çš„è®°å¿†å’ŒèŠå¤©è®°å½•å—ï¼Ÿ\nå¦‚æœ‰éœ€è¦å¯å…ˆç‚¹å‡»ğŸ”¼å¯¼å‡ºè®°å½•å†å¼€å¯æ–°å¯¹è¯"):
        output_box.delete("1.0", "end")
        lmstudio_history, ollama_history, rwkv_history, custom_history, spark_history, glm_history, lyww_history, ds_history, qwen_history, internlm_history = [], [], [], [], [], [], [], [], [], []
        with open('data/db/letta.db', 'w', encoding="utf-8") as f:
            f.write("0")
        notice("è®°å¿†å’ŒèŠå¤©è®°å½•å·²æ¸…ç©º")


def clean_chat_web():  # æ¸…é™¤å¯¹è¯è®°å½•
    global lmstudio_history, ollama_history, rwkv_history, custom_history, spark_history, glm_history, lyww_history, ds_history, qwen_history, internlm_history
    lmstudio_history, ollama_history, rwkv_history, custom_history, spark_history, glm_history, lyww_history, ds_history, qwen_history, internlm_history = [], [], [], [], [], [], [], [], [], []
